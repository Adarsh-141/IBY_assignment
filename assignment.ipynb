{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b2944bfbe17943e9b624b6b563313ae4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85bec1bd9d1c42b39c048b7506455308",
              "IPY_MODEL_ee0ac5cc225e40fd9305168196ff469a",
              "IPY_MODEL_441f8cd3a1fd4114854339be66c533c2"
            ],
            "layout": "IPY_MODEL_a671744a9f1040b0acb05b5e401e13f2"
          }
        },
        "85bec1bd9d1c42b39c048b7506455308": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ded7350fd874e44842045c17ba335bc",
            "placeholder": "​",
            "style": "IPY_MODEL_b75ba140224b436ca93091f126b0e7aa",
            "value": "Map (num_proc=4): 100%"
          }
        },
        "ee0ac5cc225e40fd9305168196ff469a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c82e149366184eb39732f2f2de3cbb22",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f19f004cd3e74fd2971b4c6d46b7451c",
            "value": 10
          }
        },
        "441f8cd3a1fd4114854339be66c533c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc691c30262a4a3ba3b474ee0101d16b",
            "placeholder": "​",
            "style": "IPY_MODEL_699e97f6be5c41d1b53eae1e3af6ea7c",
            "value": " 10/10 [00:00&lt;00:00,  7.72 examples/s]"
          }
        },
        "a671744a9f1040b0acb05b5e401e13f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ded7350fd874e44842045c17ba335bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b75ba140224b436ca93091f126b0e7aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c82e149366184eb39732f2f2de3cbb22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f19f004cd3e74fd2971b4c6d46b7451c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc691c30262a4a3ba3b474ee0101d16b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "699e97f6be5c41d1b53eae1e3af6ea7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6748725bc03546b1840ace8ef8eec98b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1ba28ce60ce74608944ca11e249df12d",
              "IPY_MODEL_ab238866c2824a9a997026f5b0c906bd",
              "IPY_MODEL_d14dcc9eb51a4ad8afbcc56a17ba9f8d"
            ],
            "layout": "IPY_MODEL_61cf639332994067b3acb1cedef86409"
          }
        },
        "1ba28ce60ce74608944ca11e249df12d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_102f7d2941514d3f8a8000f44461c734",
            "placeholder": "​",
            "style": "IPY_MODEL_562bdee0d6d14796b260f17c9260de8b",
            "value": "Unsloth: Tokenizing [&quot;text&quot;] (num_proc=6): 100%"
          }
        },
        "ab238866c2824a9a997026f5b0c906bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd77c982d4984b73b623f4d4ebf792fd",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d3d6049b8a943d2bec9c356f230abb6",
            "value": 10
          }
        },
        "d14dcc9eb51a4ad8afbcc56a17ba9f8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20ab8eb168b84043a2e5e14747afa387",
            "placeholder": "​",
            "style": "IPY_MODEL_8aed2041902c45f1a9c992e9d34dc801",
            "value": " 10/10 [00:01&lt;00:00,  8.43 examples/s]"
          }
        },
        "61cf639332994067b3acb1cedef86409": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "102f7d2941514d3f8a8000f44461c734": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "562bdee0d6d14796b260f17c9260de8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd77c982d4984b73b623f4d4ebf792fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d3d6049b8a943d2bec9c356f230abb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "20ab8eb168b84043a2e5e14747afa387": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8aed2041902c45f1a9c992e9d34dc801": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "### Block 1: Installations\n",
        "!pip install -q -U transformers\n",
        "!pip install -q -U peft\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U trl\n",
        "!pip install -q -U unsloth\n",
        "!pip install -q -U google-generativeai\n",
        "!pip install -q -U google-api-python-client\n",
        "!pip install -q -U streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrxTEPjqkoiW",
        "outputId": "3cffec55-0cf3-45f9-aaa2-d591d17ff55b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.7/348.7 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.6/273.6 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m136.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m132.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U google-generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR6nRuzyqs-f",
        "outputId": "e6514fef-bb06-449d-acec-bf8939db9fd8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.28.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.186.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.11.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (2.32.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7W4NVVAJjUD3"
      },
      "outputs": [],
      "source": [
        "### Block 2: API Key Setup\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata # Use this in Colab to securely store keys\n",
        "\n",
        "# --- REQUIRED: SET YOUR API KEYS ---\n",
        "# 1. Get your Gemini API Key at: https://aistudio.google.com/app/api_keys\n",
        "# 2. Get your Google Search API Key & CX ID at:\n",
        "#    - https://developers.google.com/custom-search/v1/overview\n",
        "#    - https://cse.google.com/cse/all\n",
        "#\n",
        "# In Google Colab, click the \"Key\" icon on the left and add these as secrets.\n",
        "# If not in Colab, just set them as environment variables.\n",
        "\n",
        "try:\n",
        "    os.environ['GOOGLE_API_KEY'] = 'AIzaSyA7u7oupJEEnjT2wzGZIBwF6N4IuIQGC5M'\n",
        "    os.environ['GOOGLE_CSE_ID'] = 'a5fef40a75a44414e'\n",
        "    os.environ['GOOGLE_SEARCH_API_KEY'] = 'AIzaSyD0R_0YIJxtuy2zSeygVK1XJZFa4f50THY'\n",
        "except Exception as e:\n",
        "    print(\"Could not load secrets from Colab. Make sure you have set:\")\n",
        "    print(\"GOOGLE_API_KEY (for Gemini)\")\n",
        "    print(\"GOOGLE_CSE_ID (for Google Search)\")\n",
        "    print(\"GOOGLE_SEARCH_API_KEY (for Google Search)\")\n",
        "    print(\"\\nOr, set them manually:\\n os.environ['GOOGLE_API_KEY'] = 'YOUR_KEY_HERE'\")\n",
        "    # os.environ['GOOGLE_API_KEY'] = 'YOUR_KEY_HERE'\n",
        "    # os.environ['GOOGLE_CSE_ID'] = 'YOUR_KEY_HERE'\n",
        "    # os.environ['GOOGLE_SEARCH_API_KEY'] = 'YOUR_KEY_HERE'\n",
        "\n",
        "\n",
        "# Configure the Gemini client\n",
        "if 'GOOGLE_API_KEY' in os.environ:\n",
        "    genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
        "else:\n",
        "    print(\"Gemini API Key not found. The agent will not work.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Block 3: The Fine-Tuning Dataset\n",
        "import json\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# 1. Define our curated list of (concept, problem) pairs.\n",
        "# This is our \"ground truth\" for the specialist model.\n",
        "concept_to_problem_map = {\n",
        "    \"Hash Map\": {\"problem\": \"Two Sum\", \"leetcode_id\": 1, \"youtube_query\": \"Two Sum leetcode solution\"},\n",
        "    \"Two Pointers\": {\"problem\": \"Valid Palindrome\", \"leetcode_id\": 125, \"youtube_query\": \"Valid Palindrome two pointers\"},\n",
        "    \"Sliding Window\": {\"problem\": \"Best Time to Buy and Sell Stock\", \"leetcode_id\": 121, \"youtube_query\": \"Best Time to Buy and Sell Stock sliding window\"},\n",
        "    \"Stack\": {\"problem\": \"Valid Parentheses\", \"leetcode_id\": 20, \"youtube_query\": \"Valid Parentheses stack solution\"},\n",
        "    \"Binary Search\": {\"problem\": \"Binary Search\", \"leetcode_id\": 704, \"youtube_query\": \"Binary Search algorithm tutorial\"},\n",
        "    \"Linked List\": {\"problem\": \"Reverse Linked List\", \"leetcode_id\": 206, \"youtube_query\": \"Reverse Linked List iterative solution\"},\n",
        "    \"Recursion\": {\"problem\": \"Climbing Stairs\", \"leetcode_id\": 70, \"youtube_query\": \"Climbing Stairs dynamic programming\"},\n",
        "    \"Efficient Sorting\": {\"problem\": \"Sort an Array\", \"leetcode_id\": 912, \"youtube_query\": \"Merge Sort algorithm\"},\n",
        "    \"Trees / DFS\": {\"problem\": \"Invert Binary Tree\", \"leetcode_id\": 226, \"youtube_query\": \"Invert Binary Tree DFS solution\"},\n",
        "    \"Graphs / BFS\": {\"problem\": \"Number of Islands\", \"leetcode_id\": 200, \"youtube_query\": \"Number of Islands BFS graph algorithm\"},\n",
        "}\n",
        "\n",
        "# 2. We must format this data into a \"chat\" format for the instruct-tuned model.\n",
        "# The model (Phi-3-mini) expects a specific prompt template.\n",
        "# We will use Unsloth which handles this automatically.\n",
        "# We just need to provide the data in a \"messages\" format.\n",
        "\n",
        "data_list = []\n",
        "for concept, details in concept_to_problem_map.items():\n",
        "    # The \"input\" from the user (our tool)\n",
        "    user_prompt = f\"Concept: {concept}\"\n",
        "\n",
        "    # The \"output\" we want the model to learn to generate\n",
        "    assistant_response = json.dumps({\n",
        "        \"problem_name\": details[\"problem\"],\n",
        "        \"leetcode_id\": details[\"leetcode_id\"],\n",
        "        \"youtube_query\": details[\"youtube_query\"]\n",
        "    })\n",
        "\n",
        "    # Add to our list in the \"messages\" format\n",
        "    data_list.append({\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "            {\"role\": \"assistant\", \"content\": assistant_response}\n",
        "        ]\n",
        "    })\n",
        "\n",
        "# 3. Load into a Hugging Face Dataset\n",
        "tuning_dataset = Dataset.from_list(data_list)\n",
        "\n",
        "print(\"--- Example Data Point ---\")\n",
        "print(tuning_dataset[0]['messages'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL9yhHgQjbpx",
        "outputId": "35f82ec6-1b51-4700-80cb-55e8af47d483"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Example Data Point ---\n",
            "[{'content': 'Concept: Hash Map', 'role': 'user'}, {'content': '{\"problem_name\": \"Two Sum\", \"leetcode_id\": 1, \"youtube_query\": \"Two Sum leetcode solution\"}', 'role': 'assistant'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Block 4: Running the PEFT (LoRA) Fine-Tuning\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "# Check for GPU\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"=\"*80)\n",
        "    print(\"WARNING: GPU not detected. Training will be extremely slow or fail.\")\n",
        "    print(\"Please go to Runtime > Change runtime type > and select a GPU (T4, V100, etc.)\")\n",
        "    print(\"=\"*80)\n",
        "else:\n",
        "    print(\"GPU detected. Proceeding with 4-bit QLoRA tuning.\")\n",
        "\n",
        "    # 1. Load the base model using Unsloth\n",
        "    max_seq_length = 1024\n",
        "    dtype = None\n",
        "    load_in_4bit = True\n",
        "\n",
        "    model_name = \"unsloth/Phi-3-mini-4k-instruct\"\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = model_name,\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "\n",
        "    # 1.5 Set the chat template on the tokenizer\n",
        "    # We still need this to format our data\n",
        "    tokenizer = get_chat_template(\n",
        "        tokenizer,\n",
        "        chat_template = \"phi-3\",\n",
        "    )\n",
        "\n",
        "    ### <<< THIS IS THE NEW, ROBUST SOLUTION (SECTION 1.6) ###\n",
        "    # 1.6 Pre-process the dataset manually\n",
        "    # We will create a new column called \"text\" that has the\n",
        "    # fully formatted chat string.\n",
        "\n",
        "    def format_chat_template(example):\n",
        "        # example['messages'] is a list of dicts: [{\"role\": \"user\", \"content\": ...}, ...]\n",
        "        # apply_chat_template will convert this to the correct Phi-3 format:\n",
        "        # \"<|user|>...<|end|><|assistant|>...<|end|>\"\n",
        "        formatted_text = tokenizer.apply_chat_template(\n",
        "            example['messages'],\n",
        "            tokenize = False,\n",
        "            add_generation_prompt = False # We are training on the full chat\n",
        "        )\n",
        "        return {\"text\": formatted_text} # Return a dict with the new column\n",
        "\n",
        "    # Apply this function to our entire dataset\n",
        "    processed_dataset = tuning_dataset.map(\n",
        "        format_chat_template,\n",
        "        num_proc = 4, # Use 4 processes to speed this up\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Example of processed data ---\")\n",
        "    print(processed_dataset[0]['text'])\n",
        "    print(\"---------------------------------\")\n",
        "\n",
        "    ### <<< END OF NEW SOLUTION (SECTION 1.6) ###\n",
        "\n",
        "\n",
        "    # 2. Configure PEFT (LoRA)\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r = 16,\n",
        "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                          \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        lora_alpha = 16,\n",
        "        lora_dropout = 0,\n",
        "        bias = \"none\",\n",
        "        use_gradient_checkpointing = True,\n",
        "        random_state = 42,\n",
        "    )\n",
        "\n",
        "    # 3. Set up the Trainer\n",
        "    trainer = SFTTrainer(\n",
        "        model = model,\n",
        "        tokenizer = tokenizer,\n",
        "\n",
        "        ### <<< THIS IS THE NEW, SIMPLIFIED SETUP (SECTION 3) ###\n",
        "        train_dataset = processed_dataset, # Pass our new dataset\n",
        "        dataset_text_field = \"text\",     # Tell the trainer to use the \"text\" column\n",
        "        # We NO LONGER need formatting_func or packing.\n",
        "        ### <<< END OF NEW SETUP (SECTION 3) ###\n",
        "\n",
        "        max_seq_length = max_seq_length,\n",
        "\n",
        "        args = TrainingArguments(\n",
        "            report_to = \"none\",\n",
        "            per_device_train_batch_size = 2,\n",
        "            gradient_accumulation_steps = 4,\n",
        "            warmup_steps = 5,\n",
        "            num_train_epochs = 20,\n",
        "            learning_rate = 2e-4,\n",
        "            fp16 = not torch.cuda.is_bf16_supported(),\n",
        "            bf16 = torch.cuda.is_bf16_supported(),\n",
        "            logging_steps = 1,\n",
        "            optim = \"adamw_8bit\",\n",
        "            weight_decay = 0.01,\n",
        "            lr_scheduler_type = \"linear\",\n",
        "            seed = 42,\n",
        "            output_dir = \"outputs\",\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # 4. Run the training!\n",
        "    print(\"--- Starting Fine-Tuning ---\")\n",
        "    trainer.train()\n",
        "    print(\"--- Fine-Tuning Complete ---\")\n",
        "\n",
        "    # 5. Save the trained LoRA adapters\n",
        "    lora_model_path = \"code_mentor_lora_adapter\"\n",
        "    trainer.save_model(lora_model_path)\n",
        "\n",
        "    print(f\"LoRA adapters saved to: {lora_model_path}\")\n",
        "\n",
        "    # 6. Test inference (optional, but good practice)\n",
        "    print(\"\\n--- Testing Tuned Model ---\")\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": \"Concept: Binary Search\"}\n",
        "    ]\n",
        "    inputs = tokenizer.apply_chat_template(messages, tokenize = True, add_generation_prompt = True, return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(input_ids = inputs, max_new_tokens = 128, use_cache = True)\n",
        "    response = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "    print(\"Test Input: Concept: Binary Search\")\n",
        "    print(\"Test Output:\")\n",
        "    print(response)\n",
        "\n",
        "    # Clean up memory\n",
        "    del model, trainer, tokenizer\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b2944bfbe17943e9b624b6b563313ae4",
            "85bec1bd9d1c42b39c048b7506455308",
            "ee0ac5cc225e40fd9305168196ff469a",
            "441f8cd3a1fd4114854339be66c533c2",
            "a671744a9f1040b0acb05b5e401e13f2",
            "4ded7350fd874e44842045c17ba335bc",
            "b75ba140224b436ca93091f126b0e7aa",
            "c82e149366184eb39732f2f2de3cbb22",
            "f19f004cd3e74fd2971b4c6d46b7451c",
            "dc691c30262a4a3ba3b474ee0101d16b",
            "699e97f6be5c41d1b53eae1e3af6ea7c",
            "6748725bc03546b1840ace8ef8eec98b",
            "1ba28ce60ce74608944ca11e249df12d",
            "ab238866c2824a9a997026f5b0c906bd",
            "d14dcc9eb51a4ad8afbcc56a17ba9f8d",
            "61cf639332994067b3acb1cedef86409",
            "102f7d2941514d3f8a8000f44461c734",
            "562bdee0d6d14796b260f17c9260de8b",
            "fd77c982d4984b73b623f4d4ebf792fd",
            "0d3d6049b8a943d2bec9c356f230abb6",
            "20ab8eb168b84043a2e5e14747afa387",
            "8aed2041902c45f1a9c992e9d34dc801"
          ]
        },
        "id": "i0vHEbdAkgNF",
        "outputId": "899e9edc-3828-490a-d7ef-420d0b14246b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU detected. Proceeding with 4-bit QLoRA tuning.\n",
            "==((====))==  Unsloth 2025.10.12: Fast Mistral patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=4):   0%|          | 0/10 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2944bfbe17943e9b624b6b563313ae4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example of processed data ---\n",
            "<|user|>\n",
            "Concept: Hash Map<|end|>\n",
            "<|assistant|>\n",
            "{\"problem_name\": \"Two Sum\", \"leetcode_id\": 1, \"youtube_query\": \"Two Sum leetcode solution\"}<|end|>\n",
            "\n",
            "---------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/10 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6748725bc03546b1840ace8ef8eec98b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Fine-Tuning ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 10 | Num Epochs = 20 | Total steps = 40\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 29,884,416 of 3,850,963,968 (0.78% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [40/40 00:50, Epoch 20/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.832500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.125800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.917100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.507600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.501800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.368200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.013200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.392700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.250400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.138400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.957900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.555700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.620000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.587100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.424400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.570900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.334700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.299000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.221700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.286200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.196600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.122800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.133300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.160100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.105700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.119300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.096200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.068100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.069900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.100400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.075900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.065400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.075800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.066800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.065600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.062800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.062800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.058900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.062900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.054900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Fine-Tuning Complete ---\n",
            "LoRA adapters saved to: code_mentor_lora_adapter\n",
            "\n",
            "--- Testing Tuned Model ---\n",
            "Test Input: Concept: Binary Search\n",
            "Test Output:\n",
            "<|user|> Concept: Binary Search<|end|><|assistant|> {\"problem_name\": \"Binary Search\", \"leetcode_id\": 704, \"youtube_query\": \"Binary Search algorithm tutorial\"}<|end|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Block 5: Agent Tools Implementation (Saving to 'agent_tools.py')\n",
        "\n",
        "%%writefile agent_tools.py\n",
        "# This magic command saves the cell contents to a file named 'agent_tools.py'\n",
        "\n",
        "import os\n",
        "import json\n",
        "import google.generativeai as genai\n",
        "from googleapiclient.discovery import build\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# --- Configuration ---\n",
        "GEMINI_API_KEY = os.environ.get('GOOGLE_API_KEY')\n",
        "SEARCH_API_KEY = os.environ.get('GOOGLE_SEARCH_API_KEY')\n",
        "CSE_ID = os.environ.get('GOOGLE_CSE_ID')\n",
        "\n",
        "# Configure Gemini model for concept extraction\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "concept_extractor_model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
        "\n",
        "# --- Tool 1: Google Search ---\n",
        "def google_search_tool(query: str) -> str:\n",
        "    \"\"\"\n",
        "    A tool to search Google. Takes a query and returns the first result URL and snippet.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        service = build(\"customsearch\", \"v1\", developerKey=SEARCH_API_KEY)\n",
        "        res = (\n",
        "            service.cse()\n",
        "            .list(\n",
        "                q=query,\n",
        "                cx=CSE_ID,\n",
        "                num=1,\n",
        "            )\n",
        "            .execute()\n",
        "        )\n",
        "\n",
        "        if \"items\" in res and len(res[\"items\"]) > 0:\n",
        "            first_result = res[\"items\"][0]\n",
        "            return json.dumps({\n",
        "                \"url\": first_result[\"link\"],\n",
        "                \"snippet\": first_result[\"snippet\"]\n",
        "            })\n",
        "        else:\n",
        "            return json.dumps({\"url\": \"Not found\", \"snippet\": \"No results found.\"})\n",
        "    except Exception as e:\n",
        "        print(f\"Error in google_search_tool: {e}\")\n",
        "        return json.dumps({\"url\": \"Error\", \"snippet\": str(e)})\n",
        "\n",
        "\n",
        "# --- Tool 2: The Fine-Tuned Model Loader ---\n",
        "# This class will load our trained LoRA adapter from Block 4.\n",
        "# We make it a class to avoid loading the model every time the tool is called.\n",
        "class SpecialistModel:\n",
        "    def __init__(self, adapter_path=\"code_mentor_lora_adapter\"):\n",
        "        self.adapter_path = adapter_path\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.load_model()\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Loads the base model and merges the LoRA adapter.\"\"\"\n",
        "        if not torch.cuda.is_available():\n",
        "            print(\"WARNING: SpecialistModel running on CPU. This will be slow.\")\n",
        "            self.device = \"cpu\"\n",
        "            load_in_4bit = False\n",
        "        else:\n",
        "            self.device = \"cuda\"\n",
        "            load_in_4bit = True\n",
        "\n",
        "        try:\n",
        "            # Load the base model\n",
        "            self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
        "                model_name = \"unsloth/Phi-3-mini-4k-instruct\",\n",
        "                max_seq_length = 1024,\n",
        "                load_in_4bit = load_in_4bit,\n",
        "            )\n",
        "\n",
        "            # Apply the LoRA adapter\n",
        "            print(f\"Loading LoRA adapter from: {self.adapter_path}\")\n",
        "            self.model = FastLanguageModel.from_pretrained(\n",
        "                self.model,\n",
        "                self.adapter_path,\n",
        "            )\n",
        "\n",
        "            # Setup tokenizer\n",
        "            from unsloth.chat_templates import get_chat_template\n",
        "            self.tokenizer = get_chat_template(\n",
        "                self.tokenizer,\n",
        "                chat_template = \"phi-3\",\n",
        "            )\n",
        "            print(\"SpecialistModel loaded successfully.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading specialist model: {e}\")\n",
        "            print(\"THIS IS A CRITICAL ERROR. THE AGENT WILL NOT WORK.\")\n",
        "            print(\"Did you run Block 4 to train and save the adapter?\")\n",
        "            self.model = None\n",
        "\n",
        "    def get_recommendation(self, concept: str) -> str:\n",
        "        \"\"\"\n",
        "        Gets a recommendation from the fine-tuned LoRA model.\n",
        "        Input: \"Binary Search\"\n",
        "        Output: '{\"problem_name\": \"Binary Search\", \"leetcode_id\": 704, ...}'\n",
        "        \"\"\"\n",
        "        if not self.model:\n",
        "            return json.dumps({\"error\": \"Specialist model not loaded.\"})\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": f\"Concept: {concept}\"}\n",
        "        ]\n",
        "\n",
        "        inputs = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize = True,\n",
        "            add_generation_prompt = True,\n",
        "            return_tensors = \"pt\"\n",
        "        ).to(self.device)\n",
        "\n",
        "        outputs = self.model.generate(input_ids = inputs, max_new_tokens = 128, use_cache = True)\n",
        "\n",
        "        # Decode *only* the new tokens\n",
        "        response_json = self.tokenizer.batch_decode(outputs[:, inputs.shape[1]:], skip_special_tokens=True)[0]\n",
        "\n",
        "        # Clean up the output string (it might have an EOS token)\n",
        "        response_json = response_json.split(\"</s>\")[0].strip()\n",
        "\n",
        "        return response_json\n",
        "\n",
        "# --- Tool 3: The Main Analysis Tool (The Pipeline) ---\n",
        "# This is the tool the Orchestrator (Gemini) will actually call.\n",
        "# It uses the other components internally.\n",
        "\n",
        "def code_analysis_and_recommendation_tool(code_snippet: str) -> str:\n",
        "    \"\"\"\n",
        "    Analyzes a user's code snippet for mistakes or inefficiencies.\n",
        "    1. Extracts the core concept using Gemini.\n",
        "    2. Gets a LeetCode problem from the fine-tuned specialist model.\n",
        "    3. Finds the URLs using the Google Search tool.\n",
        "    Returns a formatted string with the analysis and recommendations.\n",
        "    \"\"\"\n",
        "    print(f\"\\n[Tool Call: code_analysis_and_recommendation_tool]\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Extract concept with Gemini\n",
        "        prompt = f\"\"\"\n",
        "        Analyze the following code snippet. Identify the primary algorithmic mistake or\n",
        "        data structure inefficiency.\n",
        "\n",
        "        Respond with ONLY the name of the core DSA concept the user should learn.\n",
        "\n",
        "        Examples:\n",
        "        - If they use a slow O(n^2) sort -> \"Efficient Sorting\"\n",
        "        - If they use a list for O(n) lookups where a dict would be O(1) -> \"Hash Map\"\n",
        "        - If they miss a recursion base case -> \"Recursion\"\n",
        "        - If they have an off-by-one in an array -> \"Two Pointers\"\n",
        "        - If they are processing a subarray inefficiently -> \"Sliding Window\"\n",
        "\n",
        "        CODE:\n",
        "        {code_snippet}\n",
        "\n",
        "        CONCEPT:\n",
        "        \"\"\"\n",
        "        print(\"...Step 1: Extracting concept with Gemini...\")\n",
        "        concept_response = concept_extractor_model.generate_content(prompt)\n",
        "        concept = concept_response.text.strip().replace('\"', '').replace(\"'\", \"\")\n",
        "        print(f\"...Concept found: {concept}\")\n",
        "\n",
        "        # Step 2: Get recommendation from Specialist (if not loaded, initialize it)\n",
        "        # This is a simple way to get a singleton instance\n",
        "        if not hasattr(code_analysis_and_recommendation_tool, \"specialist\"):\n",
        "            print(\"...Initializing SpecialistModel (one-time setup)...\")\n",
        "            code_analysis_and_recommendation_tool.specialist = SpecialistModel()\n",
        "\n",
        "        specialist = code_analysis_and_recommendation_tool.specialist\n",
        "\n",
        "        print(f\"...Step 2: Getting recommendation from specialist for '{concept}'...\")\n",
        "        rec_json = specialist.get_recommendation(concept)\n",
        "\n",
        "        try:\n",
        "            rec = json.loads(rec_json)\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"...Error: Specialist returned invalid JSON: {rec_json}\")\n",
        "            return \"My specialist model gave me a badly formatted recommendation. Please try again.\"\n",
        "\n",
        "        if \"error\" in rec:\n",
        "            return f\"Error from specialist model: {rec['error']}\"\n",
        "\n",
        "        problem_name = rec.get(\"problem_name\", \"Unknown Problem\")\n",
        "        youtube_query = rec.get(\"youtube_query\", f\"{problem_name} solution\")\n",
        "\n",
        "        # Step 3: Get URLs with Google Search Tool\n",
        "        print(\"...Step 3: Searching for links...\")\n",
        "        # Search for LeetCode link\n",
        "        leetcode_query = f\"LeetCode problem {problem_name}\"\n",
        "        leetcode_search_res = json.loads(google_search_tool(leetcode_query))\n",
        "        leetcode_url = leetcode_search_res.get(\"url\", \"URL not found\")\n",
        "\n",
        "        # Search for YouTube link\n",
        "        youtube_search_res = json.loads(google_search_tool(youtube_query))\n",
        "        youtube_url = youtube_search_res.get(\"url\", \"URL not found\")\n",
        "\n",
        "        # Step 4: Assemble the final response for the Orchestrator\n",
        "        final_response = f\"\"\"\n",
        "        ANALYSIS:\n",
        "        I analyzed your code and identified a key concept to focus on: **{concept}**.\n",
        "\n",
        "        RECOMMENDATION:\n",
        "        A great way to practice this is by solving the **{problem_name}** problem on LeetCode.\n",
        "\n",
        "        RESOURCES:\n",
        "        - **LeetCode Problem:** {leetcode_url}\n",
        "        - **YouTube Tutorial:** {youtube_url}\n",
        "        \"\"\"\n",
        "        print(\"...Tool execution complete.\")\n",
        "        return final_response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in code_analysis_and_recommendation_tool: {e}\")\n",
        "        return f\"An error occurred during analysis: {e}\"\n",
        "\n",
        "# --- Tool 4: General Chat Tool ---\n",
        "def general_coding_question_tool(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Use this tool for general coding questions, to explain concepts,\n",
        "    or to discuss approaches. Do NOT use this tool if the user\n",
        "    provides a code snippet and asks for help or analysis.\n",
        "    \"\"\"\n",
        "    print(f\"\\n[Tool Call: general_coding_question_tool]\")\n",
        "    try:\n",
        "        # Just call the Gemini model for a straight answer\n",
        "        chat_model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "        response = chat_model.generate_content(query)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error in general_coding_question_tool: {e}\")\n",
        "        return f\"An error occurred: {e}\"\n",
        "\n",
        "\n",
        "print(\"agent_tools.py written successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzuxpK1ykjY0",
        "outputId": "45b33d47-9e10-4fc0-e169-68e171cb05bd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting agent_tools.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Block 6: Evaluation Methodology\n",
        "\n",
        "import agent_tools\n",
        "import json\n",
        "\n",
        "print(\"--- 1. Evaluating Fine-Tuned Specialist Model ---\")\n",
        "# (This assumes the SpecialistModel class is in memory or can be imported)\n",
        "# Let's re-use the init from the tool file\n",
        "if not hasattr(agent_tools.code_analysis_and_recommendation_tool, \"specialist\"):\n",
        "     agent_tools.code_analysis_and_recommendation_tool.specialist = agent_tools.SpecialistModel()\n",
        "specialist = agent_tools.code_analysis_and_recommendation_tool.specialist\n",
        "\n",
        "test_concepts = [\n",
        "    \"Hash Map\",\n",
        "    \"Binary Search\",\n",
        "    \"Sliding Window\",\n",
        "    \"Trees / DFS\"\n",
        "]\n",
        "expected_problems = [\n",
        "    \"Two Sum\",\n",
        "    \"Binary Search\",\n",
        "    \"Best Time to Buy and Sell Stock\",\n",
        "    \"Invert Binary Tree\"\n",
        "]\n",
        "\n",
        "correct = 0\n",
        "total = len(test_concepts)\n",
        "\n",
        "if specialist.model:\n",
        "    for concept, expected in zip(test_concepts, expected_problems):\n",
        "        response_json = specialist.get_recommendation(concept)\n",
        "        try:\n",
        "            response_data = json.loads(response_json)\n",
        "            predicted_problem = response_data.get(\"problem_name\")\n",
        "            if predicted_problem == expected:\n",
        "                correct += 1\n",
        "                print(f\"✅ PASSED: {concept} -> {predicted_problem}\")\n",
        "            else:\n",
        "                print(f\"❌ FAILED: {concept} -> {predicted_problem} (Expected: {expected})\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ FAILED: {concept} -> Invalid JSON ({e})\")\n",
        "\n",
        "    print(f\"\\nSpecialist Model Accuracy: {correct / total * 100:.2f}%\")\n",
        "else:\n",
        "    print(\"Specialist model not loaded. Skipping evaluation.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- 2. Evaluating Agent (Orchestrator) Tool-Use ---\")\n",
        "# We will define our agent model and tools for Gemini\n",
        "agent_model = genai.GenerativeModel(\n",
        "    system_instruction=\"\"\"\n",
        "    You are a helpful and expert coding assistant.\n",
        "    You have two tools to answer user requests.\n",
        "    - general_coding_question_tool: Use for general questions, to explain concepts, or discuss approaches.\n",
        "    - code_analysis_and_recommendation_tool: Use ONLY when the user provides a code snippet and asks for help,\n",
        "      analysis, or says it's buggy/slow.\n",
        "\n",
        "    Reason your choice of tool, then call it.\n",
        "    \"\"\",\n",
        "    model_name='gemini-2.5-flash',\n",
        "    tools=[\n",
        "        agent_tools.general_coding_question_tool,\n",
        "        agent_tools.code_analysis_and_recommendation_tool\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define our test prompts and their expected tool\n",
        "evaluation_prompts = [\n",
        "    {\n",
        "        \"prompt\": \"Can you explain what a hash map is?\",\n",
        "        \"expected_tool\": \"general_coding_question_tool\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"What's the best way to approach the Two Sum problem?\",\n",
        "        \"expected_tool\": \"general_coding_question_tool\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"\"\"\n",
        "        My code is wrong, what's the bug?\n",
        "        def find_val(nums, target):\n",
        "            for i in range(len(nums)):\n",
        "                for j in range(len(nums)):\n",
        "                    if nums[i] + nums[j] == target:\n",
        "                        return [i, j]\n",
        "        \"\"\",\n",
        "        \"expected_tool\": \"code_analysis_and_recommendation_tool\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"I wrote this sort but it's super slow. Help me.\",\n",
        "        \"expected_tool\": \"code_analysis_and_recommendation_tool\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"Running tool-use evaluation...\")\n",
        "correct = 0\n",
        "total = len(evaluation_prompts)\n",
        "\n",
        "# We start a \"chat\" to simulate the agent\n",
        "chat = agent_model.start_chat(enable_automatic_function_calling=False)\n",
        "\n",
        "for item in evaluation_prompts:\n",
        "    prompt = item[\"prompt\"]\n",
        "    expected = item[\"expected_tool\"]\n",
        "\n",
        "    # Send the prompt and get the model's response (which should be a tool call)\n",
        "    response = chat.send_message(prompt)\n",
        "\n",
        "    try:\n",
        "        # Check if the model's response has a tool call\n",
        "        if response.parts[0].function_call:\n",
        "            called_tool = response.parts[0].function_call.name\n",
        "            if called_tool == expected:\n",
        "                correct += 1\n",
        "                print(f\"✅ PASSED: Prompt -> '{prompt[:30]}...' | Called: {called_tool}\")\n",
        "            else:\n",
        "                print(f\"❌ FAILED: Prompt -> '{prompt[:30]}...' | Called: {called_tool} (Expected: {expected})\")\n",
        "        else:\n",
        "            print(f\"❌ FAILED: Prompt -> '{prompt[:30]}...' | Called: No tool (Expected: {expected})\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ ERROR: Prompt -> '{prompt[:30]}...' | Error: {e}\")\n",
        "\n",
        "print(f\"\\nAgent Tool-Use Accuracy: {correct / total * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "Xe6IXsGmp2Zs",
        "outputId": "3c67e435-3171-46da-a9c2-9fa1edaeaa9a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Evaluating Fine-Tuned Specialist Model ---\n",
            "Specialist model not loaded. Skipping evaluation.\n",
            "\n",
            "--- 2. Evaluating Agent (Orchestrator) Tool-Use ---\n",
            "Running tool-use evaluation...\n",
            "✅ PASSED: Prompt -> 'Can you explain what a hash ma...' | Called: general_coding_question_tool\n",
            "✅ PASSED: Prompt -> 'What's the best way to approac...' | Called: general_coding_question_tool\n",
            "✅ PASSED: Prompt -> '\n",
            "        My code is wrong, wha...' | Called: code_analysis_and_recommendation_tool\n",
            "✅ PASSED: Prompt -> 'I wrote this sort but it's sup...' | Called: code_analysis_and_recommendation_tool\n",
            "\n",
            "Agent Tool-Use Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Block 7: The Streamlit App (Save as 'app.py')\n",
        "\n",
        "%%writefile app.py\n",
        "# This magic command saves the cell contents to a file named 'app.py'\n",
        "\n",
        "import streamlit as st\n",
        "import google.generativeai as genai\n",
        "import agent_tools # This imports our file from Block 5\n",
        "import os\n",
        "\n",
        "# --- Page Configuration ---\n",
        "st.set_page_config(\n",
        "    page_title=\"Code-Mentor AI Agent\",\n",
        "    page_icon=\"🤖\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "st.title(\"🤖 Code-Mentor AI Agent\")\n",
        "st.caption(\"Your personal coding assistant. Ask me to explain a concept, or give me your buggy code for analysis.\")\n",
        "\n",
        "# --- API Key Check ---\n",
        "# Check if all required API keys are set\n",
        "if not all([\n",
        "    'GOOGLE_API_KEY' in os.environ,\n",
        "    'GOOGLE_CSE_ID' in os.environ,\n",
        "    'GOOGLE_SEARCH_API_KEY' in os.environ\n",
        "]):\n",
        "    st.error(\"Missing API Keys! Please set GOOGLE_API_KEY, GOOGLE_CSE_ID, and GOOGLE_SEARCH_API_KEY as environment variables.\")\n",
        "    st.stop()\n",
        "else:\n",
        "    # We have keys, so configure Gemini\n",
        "    try:\n",
        "        genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error configuring Gemini: {e}\")\n",
        "        st.stop()\n",
        "\n",
        "\n",
        "# --- Agent Setup ---\n",
        "# Initialize the Gemini model that will act as the Orchestrator\n",
        "# It is aware of the tools available in `agent_tools.py`\n",
        "try:\n",
        "    agent_model = genai.GenerativeModel(\n",
        "        system_instruction=\"\"\"\n",
        "        You are a helpful and expert coding assistant named 'Code-Mentor'.\n",
        "        You have two tools to answer user requests.\n",
        "        1.  general_coding_question_tool: Use for general questions, to explain concepts,\n",
        "            or to discuss coding approaches.\n",
        "        2.  code_analysis_and_recommendation_tool: Use ONLY when the user provides a\n",
        "            code snippet and asks for help, analysis, or says it's buggy/slow.\n",
        "\n",
        "        - First, reason about which tool is appropriate.\n",
        "        - Then, call the chosen tool.\n",
        "        - Finally, use the tool's output to give a friendly, conversational answer.\n",
        "        - If the tool `code_analysis_and_recommendation_tool` is used,\n",
        "          format its output nicely using Markdown.\n",
        "        \"\"\",\n",
        "        model_name='gemini-2.5-flash',\n",
        "        tools=[\n",
        "            agent_tools.general_coding_question_tool,\n",
        "            agent_tools.code_analysis_and_recommendation_tool\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # We must explicitly initialize the specialist model *once*\n",
        "    # This loads the LoRA model into the tool's 'singleton'\n",
        "    with st.spinner(\"Warming up the AI Specialist... (Loading LoRA model)\"):\n",
        "        if not hasattr(agent_tools.code_analysis_and_recommendation_tool, \"specialist\"):\n",
        "            # This line points to the folder created in Block 4\n",
        "            adapter_path = \"code_mentor_lora_adapter\"\n",
        "            if not os.path.exists(adapter_path):\n",
        "                st.error(f\"Could not find LoRA adapter at '{adapter_path}'.\")\n",
        "                st.error(\"Please run the training notebook (Block 4) to create it.\")\n",
        "                st.stop()\n",
        "\n",
        "            agent_tools.code_analysis_and_recommendation_tool.specialist = agent_tools.SpecialistModel(\n",
        "                adapter_path=adapter_path\n",
        "            )\n",
        "            if not agent_tools.code_analysis_and_recommendation_tool.specialist.model:\n",
        "                st.error(\"Failed to load the Specialist Model. Check logs.\")\n",
        "                st.stop()\n",
        "\n",
        "except Exception as e:\n",
        "    st.error(f\"Failed to initialize the Agent Model: {e}\")\n",
        "    st.stop()\n",
        "\n",
        "\n",
        "# --- Chat History Setup ---\n",
        "if \"chat\" not in st.session_state:\n",
        "    # Start the chat session with the tool-enabled model\n",
        "    st.session_state.chat = agent_model.start_chat(\n",
        "        enable_automatic_function_calling=True\n",
        "    )\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display previous messages\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# --- Chat Input Logic ---\n",
        "if prompt := st.chat_input(\"Ask a question or paste your code for analysis...\"):\n",
        "\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "    # Send the prompt to the Gemini agent\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            try:\n",
        "                # The .send_message() call will automatically:\n",
        "                # 1. Reason which tool to use.\n",
        "                # 2. Call the tool (e.g., `code_analysis_and_recommendation_tool`).\n",
        "                # 3. Get the tool's string output.\n",
        "                # 4. Use that output to generate the final response.\n",
        "                response = st.session_state.chat.send_message(prompt)\n",
        "\n",
        "                # The final response from the model (after tool use)\n",
        "                final_answer = response.text\n",
        "                st.markdown(final_answer)\n",
        "                st.session_state.messages.append({\"role\": \"assistant\", \"content\": final_answer})\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"An error occurred: {e}\")\n",
        "                st.session_state.messages.append({\"role\": \"assistant\", \"content\": f\"Error: {e}\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4F9ErXLRqVEO",
        "outputId": "e9b64dae-2228-44bb-f8b0-0eeaa8066226"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzYWt7gEtnMo",
        "outputId": "10797f70-d619-4ab5-d77b-41687be93487"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.4.1-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Downloading pyngrok-7.4.1-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok  # Ensure it's installed\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "\n",
        "# --- PASTE YOUR AUTHTOKEN HERE ---\n",
        "AUTHTOKEN = \"34vuAfpudjalhxNk74mHekmSb4b_7jGZBF7GjeCJ5dkdPef2V\"\n",
        "# ---------------------------------\n",
        "\n",
        "if AUTHTOKEN == \"YOUR_NGROK_AUTHTOKEN_GOES_HERE\":\n",
        "     print(\"ERROR: Please go to https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "     print(\"and paste your ngrok authtoken into the AUTHTOKEN variable.\")\n",
        "else:\n",
        "     # Set the authtoken\n",
        "     ngrok.set_auth_token(AUTHTOKEN)\n",
        "\n",
        "     # Run streamlit in the background\n",
        "     # We use 'nohup' and '&' to make it run in the background\n",
        "     os.system(\"nohup streamlit run app.py --server.port 8501 &\")\n",
        "\n",
        "     # Connect ngrok to the Streamlit port (default 8501)\n",
        "     public_url = ngrok.connect(8501)\n",
        "     print(\"\\n🚀 Your app is live at:\")\n",
        "     print(public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upNycnZwt5BH",
        "outputId": "f3e3a149-44a8-4125-9d27-b282e8b8aa22"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.4.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "\n",
            "🚀 Your app is live at:\n",
            "NgrokTunnel: \"https://byron-subcuneus-thea.ngrok-free.dev\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7tLehanftWfp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}